# Speaker Notes Backup - AI Research Oversight Presentation
## Backup Created: 2025-01-01

### Slide 1 - Title
Thank you for joining me today. This topic is huge and, based on the title, you'd think I would have all the answers – I don't. However, what I'm hoping to offer is a unique perspective on the topic of AI research, particularly as it relates to ethical oversight.

### Slide 2 - Speaker Background
Mark Lifson brings a unique interdisciplinary perspective to AI oversight in healthcare, with a PhD from University of Rochester in Biomedical Engineering and postdoctoral fellowship at Stanford focused on photonic crystal biosensors for point-of-care diagnostics.

Industry experience includes launching FDA-approved AI products at Abbott Laboratories, developing commercialized biosensor platforms at Adarza BioSystems, and leading AI/ML engineering at Mayo Clinic's Center for Digital Health.

Key message: Combining engineering rigor from biosensor development with practical experience in FDA regulatory approval and clinical implementation to create frameworks that enable responsible AI innovation in healthcare.

### Slide 3 - Central Question
I'll start with the most basic question that I'm still trying to get answered: What causes angst about AI within research communities?

When I raise this question, many of you may be tempted to start answering right away with concerns like these. These are all valid concerns that I've heard repeatedly in discussions about AI oversight.

### Slide 4 - Not Unique to AI
But let me pause on this. At least some of the issues and sources of concern about artificial intelligence aren't actually unique to artificial intelligence. From research and regulatory perspectives, I want us to keep in mind where these concerns come from and why they matter.

These concerns are universal across many types of emerging technologies. Privacy, unintended consequences, and equitable access have been issues for decades in medical research.

### Slide 5 - AI vs Traditional
What makes AI different from traditional research paths?

In traditional research, we start with lab bench work, move to animal studies, and only then does the IRB get involved when we reach human subjects.

But AI research uses human data from the very beginning. There's no pre-human phase. This raises an important question: When should IRB oversight begin?

### Slide 6 - Clinical Phases
The three phases of clinical evaluation for AI systems mirror traditional medical device development but with unique considerations:

Phase 1: Exploratory - Algorithm development and hypothesis generation
Phase 2: Pilot/Validation - Early safety and performance testing  
Phase 3: Intervention - Clinical efficacy and patient impact

Each phase requires different validation approaches and regulatory considerations.

Press Space to reveal each phase (4 steps).

### Slide 7 - When IRB Gets Involved
This is where the practical challenge emerges for research oversight. Traditional research has clear boundaries – we know when we move from bench to bedside. But AI research exists in a gray area where data use, algorithm development, and clinical application can happen simultaneously.

Some AI research clearly requires IRB review – interventional studies where AI directly influences patient care. But other AI research falls into ambiguous territory where oversight requirements are unclear.

### Slide 8 - Regulatory Framework
The regulatory framework for AI research spans three major domains:

1. FDA Regulations - Governing medical devices and software
2. ISO Standards - International quality and safety standards
3. IRB/Ethics - Human subjects protection requirements

These regulations work together to ensure AI systems are safe, effective, and ethically developed. Key regulations include 21 CFR 820.30 for design controls, ISO 14971 for risk management, and 45 CFR 46 for human subjects protection.

Press Space to reveal the framework (10 steps).

### Slide 9 - IRB Criteria
Now we face the complex reality of IRB review criteria. Current regulations require review across multiple domains: human subjects determination, medical device regulations, safety analysis, equity considerations, consent processes, monitoring protocols, data security, and vulnerable populations.

The challenge isn't that these criteria are wrong – they're all important. The challenge is that they weren't designed with AI research in mind, leading to confusion about when and how to apply them.

### Slide 10 - Translation Gap
Here's what I see as the core problem: There's a translation gap between what AI researchers need and what IRB reviewers can provide.

AI researchers want clear guidance, practical timelines, and proportional oversight that doesn't slow innovation. IRB reviewers need understandable risk assessments, proven mitigation strategies, and assurance that human subjects are protected.

This gap creates frustration on both sides and can lead to either over-regulation that stifles innovation or under-regulation that creates risk.

### Slide 11 - Three Questions Framework
There are three main areas where I think the involvement of AI in research requires an "emphasis" rather than a whole new perspective.

After reviewing our experiences with AI research oversight, I've identified three critical questions that consistently emerge as focal points for ethical and practical evaluation.

These three questions help focus oversight where it matters most, without requiring reviewers to become AI experts.

Press Space to reveal each question (4 steps total).

### Slide 12 - Question 1
Our first key question is: Is there a clinical intended use? This question helps us determine if the research requires safety and efficacy evaluation beyond basic data privacy concerns.

### Slide 13 - Language Matters
When examining this question, language choices become critically important. The terminology researchers use signals different regulatory implications:

On the left, we see clinical language like 'diagnose,' 'treat,' 'predict,' and 'clinical decision support.' These terms suggest a clinical intended use that may trigger regulatory oversight.

On the right, we see exploratory language like 'investigate,' 'analyze,' 'examine,' and 'explore associations.' These terms suggest a research focus that may not require the same level of regulatory scrutiny.

### Slide 14 - Question 2
Our second key question addresses the spectrum of patient impact: Will this research affect patient care? This question helps determine the appropriate level of oversight and safety monitoring required.

### Slide 15 - Impact Spectrum
Patient impact exists on a spectrum from silent research tools to autonomous clinical systems. The level of IRB oversight should scale proportionally with this impact.

Silent tools that researchers use internally require minimal oversight. Advisory systems that inform clinical decisions require moderate oversight. Autonomous systems that directly control patient care require comprehensive oversight.

### Slide 16 - Question 3
Our third key question addresses what makes AI oversight uniquely challenging: What additional risks to human subjects, if any, are the result of the technology itself? This question helps identify AI-specific considerations that standard oversight processes might miss.

### Slide 17 - Technology Risks & Mitigations
This slide would contain speaker notes for technology-specific risks and mitigations content.

### Slide 18 - Risk Acceptability Matrix
Risk-Based Oversight Framework

This matrix shows how Q1 (Clinical Intent) and Q2 (Patient Impact) determine the base risk level and oversight requirements.

Q3 (Technology Risks) adds another dimension - it doesn't change the base oversight level but informs what specific mitigations are needed:
- Data quality controls
- Explainability requirements
- Human oversight mechanisms
- Reversibility procedures

The interactive panel on the right shows how technology risks accumulate and increase the overall risk profile, requiring additional safeguards.

### Slide 19 - Practical Checklist
This slide would contain speaker notes for the practical assessment tool content.

### Slide 20 - Conclusion
I want to conclude with a synthesis of everything we've covered today. These three questions – clinical intended use, patient impact, and technology-specific risks – provide a framework for evaluating AI research that builds on existing IRB processes rather than replacing them.

The goal isn't to create new bureaucracy, but to focus oversight where it's most needed. By asking these three questions, we can ensure that AI research receives appropriate ethical review while avoiding unnecessary barriers to innovation.

The future of AI in healthcare depends on getting this balance right. We need oversight that protects patients without stifling the potential for AI to improve healthcare outcomes.