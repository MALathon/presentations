# Speaker Notes - AI Research Oversight Framework
## 30-Minute Presentation Guide

---

## Slide 1: Title (1.5 minutes)
**Visual**: Title with neural network animation, both authors listed

Good morning everyone, and thank you for joining me today for this discussion on AI research oversight. 

I'm Mark Lifson, and together with my colleague Tamiko Eto, we've developed this framework to address one of the most pressing challenges in modern research oversight.

The title of today's presentation might suggest we have all the answers to this complex challenge - we don't. What we hope to offer instead is a practical framework developed from our experience at Mayo Clinic, combined with my background in both FDA-regulated device development and clinical AI implementation.

Over the next 30 minutes, we'll explore three key questions that can help streamline AI research oversight without requiring IRB reviewers to become AI experts themselves. Our goal is to provide you with actionable tools that build on existing processes rather than creating entirely new bureaucratic hurdles.

---

## Slide 2: Speaker Introduction (1.5 minutes)
**Visual**: Interactive Venn diagram (Space to toggle, B for badges, R to reset)

[Space to reveal full bio]

My journey to this topic isn't typical. I started in aerospace engineering, moved through FDA-regulated device development, and now work at the intersection of clinical AI and research oversight.

[B to show badges/credentials]

This diverse background matters because AI research oversight requires understanding multiple domains - regulatory requirements, clinical workflows, technical capabilities, and ethical implications. No single expertise is sufficient.

What I've learned is that effective oversight doesn't come from trying to become an expert in everything, but from building frameworks that leverage existing expertise while addressing what's genuinely new about AI.

---

## Slide 3: The Central Question (1.5 minutes)
**Visual**: Question with hidden concern cards (Space to reveal, R to reset)

Let me start with what I believe is the fundamental question underlying all AI oversight discussions: What causes angst about AI within research communities?

[Space to reveal concerns]

The concerns aren't actually about the technology itself. They're about control, interpretability, and the pace of change. IRBs worry they lack the expertise to evaluate AI protocols. Researchers worry about bureaucratic delays. Both groups are right to be concerned.

But here's the key insight: most of what makes AI research "research" isn't new. Human subjects protection principles don't change because we're using neural networks instead of linear regression.

---

## Slide 4: Not Unique to AI (1.5 minutes)
**Visual**: Comparison table (Space to advance, Click rows to highlight)

Let's be honest about what we're dealing with. Every concern raised about AI has precedent in other domains.

[Space to reveal each row]

Black box algorithms? We've been using complex surgical devices for decades. Bias in data? Clinical trials have struggled with representation forever. Rapid evolution? Look at genomics or immunotherapy.

The difference isn't the challenges themselves - it's that AI combines all of them simultaneously. This is why we need frameworks that build on existing knowledge rather than starting from scratch.

---

## Slide 5: What's Different About AI (1.5 minutes)
**Visual**: Traditional vs AI development comparison

The fundamental difference between traditional and AI development is the shift from explicit programming to learned behavior. 

In traditional development, we write rules. In AI, we provide examples and the system learns patterns. This changes everything about how we validate, monitor, and control these systems.

But - and this is crucial - it doesn't change the fundamental questions about human subjects protection. It just changes how we answer them.

---

## Slide 6: Clinical Development Phases (1 minute)
**Visual**: Circular AI development cycle

Unlike traditional linear development, AI systems evolve continuously. They learn from new data, adapt to local patterns, and can drift from their original training.

This circular nature isn't a bug - it's the entire point. But it means our oversight frameworks need to account for systems that change over time, sometimes in unexpected ways.

---

## Slide 7: When IRB Review is Needed (2 minutes)
**Visual**: Decision flowchart

So when does AI research require IRB review? The answer is simpler than you might think: apply existing definitions.

If you're conducting systematic investigation with living individuals to develop generalizable knowledge, it's research. The AI component might affect the level of review or specific considerations, but it doesn't change this fundamental determination.

The key is distinguishing research from quality improvement, operations, or clinical care - distinctions we already know how to make.

---

## Slide 8: Regulatory Framework (1.5 minutes)
**Visual**: FDA pathway diagram

The regulatory landscape for AI is complex but navigable. FDA has clear pathways for AI as medical devices. IRBs have existing frameworks for risk assessment.

What's missing isn't regulation - it's translation between technical and regulatory languages. That's what our three-question framework provides.

---

## Slide 9: IRB Review Criteria (2.minutes)
**Visual**: Interactive wheel (Tab to navigate, 1-8 to jump, Click categories to filter)

[Tab through criteria]

Let me show you how existing IRB criteria map to AI research. Every standard criterion applies, but some need special attention.

[Press 2 for SaMD] Software as Medical Device classification matters for risk assessment.
[Press 4 for Equity] AI can amplify biases, making equitable selection crucial.
[Press 5 for Consent] How do we explain AI decision-making to participants?
[Press 6 for Monitoring] Continuous learning requires continuous monitoring.

These aren't new requirements - they're existing requirements that need careful application to AI contexts.

---

## Slide 10: The Translation Gap (1 minute)
**Visual**: Bridge between technical and oversight teams

The real challenge isn't technical or regulatory - it's communication. Technical teams speak in terms of AUC, F1 scores, and gradient descent. IRBs speak in terms of minimal risk, beneficence, and informed consent.

We need translators, not more experts. That's what this framework provides.

---

## Slide 11: Three Key Questions (1.5 minutes)
**Visual**: Three question framework

After working with dozens of AI research protocols, we've distilled oversight into three essential questions:

1. Is this human subjects research?
2. What's the potential for impact?
3. Is the technical risk acceptable?

These questions build on each other. Each has clear criteria. Together, they provide comprehensive oversight without requiring AI expertise.

---

## Slide 12: Question 1 - Research Determination (1.5 minutes)
**Visual**: Decision criteria

Question 1: Is this human subjects research requiring IRB review?

This isn't an AI question - it's a research question. We apply standard definitions: systematic investigation, living individuals, generalizable knowledge. The AI component might affect the answers, but not the questions.

---

## Slide 13: Why Language Matters (1.5 minutes)
**Visual**: Terminology comparison

The words we use matter enormously. "Algorithm" sounds scary. "Decision support tool" sounds helpful. They might describe the same system.

We need consistent terminology that accurately conveys both capabilities and limitations without either overselling or fear-mongering.

---

## Slide 14: Question 2 - Impact Assessment (2 minutes)
**Visual**: Impact spectrum

Question 2: What is the potential for impact on human subjects?

This ranges from minimal (retrospective analysis) to significant (clinical decisions). The key is understanding not just direct impacts, but cascading effects. An AI that seems low-risk in isolation might have significant impact when integrated into clinical workflows.

---

## Slide 15: Impact Spectrum (1.5 minutes)
**Visual**: Risk levels from minimal to critical

Let me give you concrete examples:
- Minimal: Retrospective chart review for pattern identification
- Moderate: Clinical decision support with human override
- Significant: Autonomous intervention recommendations
- Critical: Closed-loop systems with direct patient control

Each level requires different oversight intensity.

---

## Slide 16: Question 3 - Technical Risk (2 minutes)
**Visual**: Technical characteristics

Question 3: Is the technical risk acceptable relative to benefits?

This is where we translate technical metrics into risk language IRBs understand. It's not about perfect performance - it's about acceptable performance for the specific use case.

---

## Slide 17: Technical Risk Categories (1.5 minutes)
**Visual**: Risk mitigation strategies

Technical risks in AI fall into predictable categories:
- Data quality and bias
- Model generalization
- Interpretability
- Failure modes
- Drift over time

For each, we have established mitigation strategies. The challenge is matching the mitigation to the risk level and use case.

---

## Slide 18: Risk Acceptability Matrix (2 minutes)
**Visual**: Interactive risk matrix (Click cells to flip, Click factors to toggle, R to reset)

[Click through matrix cells]

Risk acceptability isn't absolute - it's contextual. A 85% accuracy might be excellent for screening but unacceptable for treatment decisions. 

[Click factors]

The key factors are: severity of consequences, availability of alternatives, reversibility of decisions, and human oversight levels.

This matrix helps translate technical performance into risk-benefit language IRBs can evaluate.

---

## Slide 19: Practical Checklist (2 minutes)
**Visual**: Interactive checklist (Click items to check, R to reset, D for disclaimer)

[Click through items]

Here's your practical takeaway - a checklist that guides you through our three-question framework. Each item links to specific regulatory requirements or best practices.

[Press D to show disclaimer]

This isn't meant to replace careful review - it's meant to ensure consistent, comprehensive evaluation. Think of it as a pre-flight checklist for AI research protocols.

Use this, adapt it, improve it. The goal is practical tools, not perfect theory.

---

## Slide 20: Questions & Discussion (2 minutes)
**Visual**: Summary with QR code for contact

Thank you for your attention today. We've explored a practical three-question framework that can help streamline AI research oversight without requiring specialized AI expertise.

The key is not having all the answers, but having the right framework for asking questions.

I welcome your thoughts, questions, and experiences as we all navigate this evolving landscape together. Please feel free to reach out - I'm always interested in learning how different institutions are approaching these challenges.

[QR code on screen for LinkedIn connection]

Thank you.

---

## Time Management Summary
- Introduction & Setup: 3 minutes (Slides 1-2)
- Problem Definition: 4.5 minutes (Slides 3-5)
- Context & Framework: 4.5 minutes (Slides 6-9)
- Three Questions Deep Dive: 10 minutes (Slides 10-18)
- Practical Application: 4 minutes (Slide 19)
- Q&A Setup: 2 minutes (Slide 20)
- Buffer/Discussion: 2 minutes

Total: 30 minutes

## Key Interaction Points
- Slide 2: Space for bio reveal, B for badges
- Slide 3: Space to reveal concerns
- Slide 4: Space to advance comparison
- Slide 9: Tab/numbers for criteria navigation
- Slide 18: Click for risk matrix exploration
- Slide 19: Interactive checklist demonstration

## Delivery Notes
- Maintain conversational tone throughout
- Use interactions to pace presentation and maintain engagement
- Don't rush through technical slides - these are where value is delivered
- Leave time for questions on practical application
- Have backup examples ready for each risk category
- Be prepared to dive deeper into any of the three questions based on audience interest