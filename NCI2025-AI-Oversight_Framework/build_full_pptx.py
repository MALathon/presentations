# Create full presentation using MCP PowerPoint
slides = [
    ("Speaker Background", "• Aerospace Engineering → Medical Devices → Clinical AI\n• FDA-regulated device development experience\n• Mayo Clinic AI research oversight\n• Cross-domain perspective on safety and efficacy\n\nKey Insight: Effective oversight comes from frameworks that leverage existing expertise while addressing what's genuinely new about AI"),
    ("The Central Question", "What causes angst about AI within research communities?\n\nCore Concerns:\n• Control and predictability\n• Interpretability and explainability  \n• Pace of change and evolution\n• Expertise gap\n\nKey Insight: Most of what makes AI research 'research' isn't new"),
    ("These Challenges Are Not Unique to AI", "Black Box Algorithms → Complex Medical Devices\nBias in Data → Clinical Trial Representation\nRapid Evolution → Genomics, Immunotherapy\nRegulatory Uncertainty → Any Novel Therapy\nValidation Challenges → Personalized Medicine\n\nThe difference: AI combines all challenges simultaneously"),
    ("What's Different: Traditional vs AI", "Traditional Development:\n• Explicit programming\n• Fixed rules\n• Predictable behavior\n• Linear validation\n\nAI Development:\n• Learned behavior\n• Pattern recognition\n• Emergent properties\n• Continuous evolution"),
    ("Clinical Development Phases", "Traditional: Linear progression\n  Discovery → Development → Validation → Deployment\n\nAI: Circular evolution\n  Training ↔ Validation ↔ Deployment ↔ Retraining\n\nImplication: Oversight must account for continuous change"),
    ("When is IRB Review Required?", "Apply Existing Definitions:\n\n✓ Systematic investigation?\n✓ Living individuals?\n✓ Generalizable knowledge?\n\n= Human Subjects Research\n\nAI doesn't change the definition, just the application"),
    ("Regulatory Framework", "FDA Pathways:\n• SaMD (Software as Medical Device)\n• 510(k), De Novo, PMA routes\n• Predetermined Change Control Plans\n\nIRB Considerations:\n• Risk level determination\n• Consent requirements\n• Monitoring plans"),
    ("IRB Review Criteria Applied to AI", "1. Research vs Practice → Standard definitions apply\n2. SaMD Classification → FDA risk categories\n3. Risk/Benefit → Technical performance metrics\n4. Equitable Selection → Bias assessment\n5. Informed Consent → Explainability requirements\n6. Data Monitoring → Drift detection\n7. Privacy → Data governance\n8. Vulnerable Populations → Enhanced protections"),
    ("The Translation Gap", "Technical Teams Speak:\n• AUC, F1 scores\n• Gradient descent\n• Neural architectures\n\nIRBs Speak:\n• Minimal risk\n• Beneficence\n• Informed consent\n\nWe need translators, not more experts"),
    ("Three Key Questions Framework", "1. Is this human subjects research?\n   → Apply standard definitions\n\n2. What is the potential for impact?\n   → Assess direct and cascading effects\n\n3. Is the technical risk acceptable?\n   → Translate metrics to risk language"),
    ("Question 1: Research Determination", "Is this human subjects research requiring IRB review?\n\nApply Standard Criteria:\n• Systematic investigation\n• Living individuals  \n• Generalizable knowledge\n\nAI-Specific Considerations:\n• Training vs deployment\n• Quality improvement vs research"),
    ("Why Language Matters", "'Algorithm' → Scary\n'Decision Support Tool' → Helpful\n\nSame system, different perceptions\n\nNeed consistent terminology that:\n• Accurately conveys capabilities\n• Acknowledges limitations\n• Avoids fear-mongering"),
    ("Question 2: Impact Assessment", "What is the potential for impact on human subjects?\n\nConsider:\n• Direct impacts (immediate effects)\n• Indirect impacts (workflow changes)\n• Cascading effects (system-wide changes)\n• Reversibility of decisions\n• Human oversight levels"),
    ("Impact Spectrum", "Minimal Risk:\n• Retrospective analysis\n• Pattern identification\n\nModerate Risk:\n• Clinical decision support\n• Human override required\n\nSignificant Risk:\n• Autonomous recommendations\n• Treatment decisions"),
    ("Question 3: Technical Risk", "Is the technical risk acceptable relative to benefits?\n\nKey Considerations:\n• Performance metrics in context\n• Failure modes and mitigation\n• Bias and fairness assessment\n• Interpretability requirements\n• Monitoring and maintenance plans"),
    ("Technical Risk Categories", "Data Risks:\n• Quality, bias, representation\n\nModel Risks:\n• Generalization, overfitting, drift\n\nImplementation Risks:\n• Integration, usability, maintenance\n\nEach requires specific mitigation strategies"),
    ("Risk Acceptability Matrix", "Context Determines Acceptability:\n\n85% Accuracy:\n✓ Acceptable for screening\n✗ Unacceptable for treatment\n\nKey Factors:\n• Severity of consequences\n• Availability of alternatives\n• Reversibility of decisions"),
    ("Practical Checklist", "Pre-Review Checklist:\n\n□ Research determination complete\n□ Risk level assessed\n□ Technical metrics translated\n□ Bias assessment documented\n□ Monitoring plan defined\n□ Consent language clear\n□ Data governance specified\n□ Failure modes identified"),
    ("Questions?", "A practical three-question framework for AI research oversight\n\n• Is this human subjects research requiring IRB review?\n• What is the potential for impact on human subjects?\n• Is the technical risk acceptable relative to benefits?\n\nContact:\nlifson.mark@mayo.edu\nlinkedin.com/in/marklifson")
]

print(f"Total slides to create: {len(slides)}")
for i, (title, content) in enumerate(slides, 2):
    print(f"Slide {i}: {title[:30]}...")
